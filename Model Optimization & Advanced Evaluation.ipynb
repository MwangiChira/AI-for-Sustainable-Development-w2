{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5929afb-74cd-4826-8e9a-c43343517594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully.\n",
      "Dataset head:\n",
      "   User_ID  Age Gender  Social_Media_Hours  Exercise_Hours  Sleep_Hours  \\\n",
      "0        1   16      F            9.654486        2.458001     5.198926   \n",
      "1        2   17      M            9.158143        0.392095     8.866097   \n",
      "2        3   15      M            5.028755        0.520119     4.943095   \n",
      "3        4   17      F            7.951103        1.022630     5.262773   \n",
      "4        5   17      F            1.357459        1.225462     6.196080   \n",
      "\n",
      "   Screen_Time_Hours  Survey_Stress_Score  Wearable_Stress_Score  \\\n",
      "0           8.158189                    3               0.288962   \n",
      "1           5.151993                    5               0.409446   \n",
      "2           9.209325                    2               0.423837   \n",
      "3           9.823658                    5               0.666021   \n",
      "4          11.338990                    5               0.928060   \n",
      "\n",
      "  Support_System Academic_Performance  \n",
      "0       Moderate            Excellent  \n",
      "1       Moderate                 Good  \n",
      "2       Moderate                 Poor  \n",
      "3       Moderate              Average  \n",
      "4           High                 Poor  \n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   User_ID                5000 non-null   int64  \n",
      " 1   Age                    5000 non-null   int64  \n",
      " 2   Gender                 5000 non-null   object \n",
      " 3   Social_Media_Hours     5000 non-null   float64\n",
      " 4   Exercise_Hours         5000 non-null   float64\n",
      " 5   Sleep_Hours            5000 non-null   float64\n",
      " 6   Screen_Time_Hours      5000 non-null   float64\n",
      " 7   Survey_Stress_Score    5000 non-null   int64  \n",
      " 8   Wearable_Stress_Score  5000 non-null   float64\n",
      " 9   Support_System         5000 non-null   object \n",
      " 10  Academic_Performance   5000 non-null   object \n",
      "dtypes: float64(5), int64(3), object(3)\n",
      "memory usage: 429.8+ KB\n",
      "Error: Target variable 'mental_health_condition' not found in the dataset columns.\n",
      "Available columns: ['User_ID', 'Age', 'Gender', 'Social_Media_Hours', 'Exercise_Hours', 'Sleep_Hours', 'Screen_Time_Hours', 'Survey_Stress_Score', 'Wearable_Stress_Score', 'Support_System', 'Academic_Performance']\n",
      "Please specify the correct target variable.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['mental_health_condition'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m     exit()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Separate features (X) and target (y)\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtarget_variable\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m y \u001b[38;5;241m=\u001b[39m df[target_variable]\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Identify categorical and numerical features\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages/pandas/core/frame.py:5344\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5197\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5198\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5205\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5206\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5207\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5208\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5209\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5342\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5343\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5346\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5350\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5351\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5352\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages/pandas/core/generic.py:4711\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4709\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4710\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4711\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4714\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages/pandas/core/generic.py:4753\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4751\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4752\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4753\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4754\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4756\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4757\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/envs/anaconda-2024.02-py310/lib/python3.10/site-packages/pandas/core/indexes/base.py:7000\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6998\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7000\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7001\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7002\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['mental_health_condition'] not found in axis\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, RocCurveDisplay\n",
    "\n",
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('mental_health_analysis.csv')\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "    print(\"Dataset head:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nDataset info:\")\n",
    "    df.info()\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: mental_health_analysis.csv not found. Please ensure the file is in the correct directory.\")\n",
    "    exit()\n",
    "\n",
    "# --- 1. Data Loading and Preprocessing ---\n",
    "\n",
    "# Identify target variable (assuming 'mental_health_condition' as target for classification)\n",
    "# If your target variable is different, please adjust 'target_variable'.\n",
    "# If it's a regression task, specify that as well.\n",
    "target_variable = 'mental_health_condition' # This is an assumption, please adjust if needed\n",
    "\n",
    "# Check if the target variable exists in the DataFrame\n",
    "if target_variable not in df.columns:\n",
    "    print(f\"Error: Target variable '{target_variable}' not found in the dataset columns.\")\n",
    "    print(f\"Available columns: {df.columns.tolist()}\")\n",
    "    print(\"Please specify the correct target variable.\")\n",
    "    exit()\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = df.drop(columns=[target_variable])\n",
    "y = df[target_variable]\n",
    "\n",
    "# Identify categorical and numerical features\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_features = X.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Preprocessing pipelines for numerical and categorical features\n",
    "numerical_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "# Create a preprocessor using ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y if y.nunique() > 2 else None)\n",
    "# stratify is used for classification to maintain class distribution\n",
    "\n",
    "print(f\"\\nTraining set shape: {X_train.shape}, Test set shape: {X_test.shape}\")\n",
    "\n",
    "# --- 2. Baseline Model Training ---\n",
    "print(\"\\n--- Baseline Model Training (RandomForestClassifier) ---\")\n",
    "\n",
    "# Define the baseline model pipeline\n",
    "# Using RandomForestClassifier as an example.\n",
    "# If you used a different model (e.g., LogisticRegression, SVC) in Task 3, replace it here.\n",
    "baseline_model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                          ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Train the baseline model\n",
    "baseline_model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate baseline model\n",
    "y_pred_baseline = baseline_model_pipeline.predict(X_test)\n",
    "print(\"\\nBaseline Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_baseline))\n",
    "\n",
    "# Store baseline performance for comparison\n",
    "baseline_report = classification_report(y_test, y_pred_baseline, output_dict=True)\n",
    "baseline_accuracy = baseline_report['accuracy']\n",
    "print(f\"Baseline Model Accuracy: {baseline_accuracy:.4f}\")\n",
    "\n",
    "# --- 3. Hyperparameter Tuning (GridSearchCV) ---\n",
    "print(\"\\n--- Hyperparameter Tuning (GridSearchCV for RandomForestClassifier) ---\")\n",
    "\n",
    "# Define the model to be tuned (same as baseline, but now with tuning)\n",
    "tuned_model_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                                        ('classifier', RandomForestClassifier(random_state=42))])\n",
    "\n",
    "# Define the hyperparameter grid for RandomForestClassifier\n",
    "# You can customize these parameters based on your understanding of the model and dataset\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [100, 200, 300], # Number of trees in the forest\n",
    "    'classifier__max_features': ['sqrt', 'log2'], # Number of features to consider when looking for the best split\n",
    "    'classifier__max_depth': [10, 20, 30, None], # Maximum number of levels in a tree\n",
    "    'classifier__min_samples_split': [2, 5, 10], # Minimum number of data points placed in a node before the node is split\n",
    "    'classifier__min_samples_leaf': [1, 2, 4] # Minimum number of data points allowed in a leaf node\n",
    "}\n",
    "\n",
    "# Set up GridSearchCV\n",
    "# n_jobs=-1 uses all available CPU cores for parallel processing\n",
    "grid_search = GridSearchCV(tuned_model_pipeline, param_grid, cv=5, n_jobs=-1, verbose=2, scoring='accuracy')\n",
    "\n",
    "print(\"Starting GridSearchCV...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nGridSearchCV complete.\")\n",
    "print(f\"Best hyperparameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# --- 4. Improved Model Training with Best Hyperparameters ---\n",
    "print(\"\\n--- Improved Model Training with Best Hyperparameters ---\")\n",
    "\n",
    "optimized_model = grid_search.best_estimator_\n",
    "y_pred_optimized = optimized_model.predict(X_test)\n",
    "\n",
    "print(\"\\nOptimized Model Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_optimized))\n",
    "\n",
    "# Store optimized performance\n",
    "optimized_report = classification_report(y_test, y_pred_optimized, output_dict=True)\n",
    "optimized_accuracy = optimized_report['accuracy']\n",
    "print(f\"Optimized Model Accuracy: {optimized_accuracy:.4f}\")\n",
    "\n",
    "# --- 5. Advanced Evaluation Metrics and Visualizations ---\n",
    "print(\"\\n--- Advanced Evaluation Metrics and Visualizations ---\")\n",
    "\n",
    "# 5.1. Confusion Matrix\n",
    "print(\"\\nConfusion Matrix (Optimized Model):\")\n",
    "cm = confusion_matrix(y_test, y_pred_optimized)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
    "            xticklabels=optimized_model.classes_, yticklabels=optimized_model.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Optimized Model')\n",
    "plt.show()\n",
    "\n",
    "# 5.2. ROC Curve and AUC (for binary classification)\n",
    "# If your target variable has more than 2 unique values, this section needs adjustment for multi-class ROC.\n",
    "# For simplicity, if it's multi-class, we'll focus on macro/weighted average AUC from classification report.\n",
    "if len(np.unique(y)) == 2:\n",
    "    print(\"\\nROC Curve (Optimized Model):\")\n",
    "    y_prob_optimized = optimized_model.predict_proba(X_test)[:, 1] # Probability of the positive class\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_prob_optimized, pos_label=optimized_model.classes_[1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Alternative using RocCurveDisplay (more modern approach)\n",
    "    print(\"\\nROC Curve using RocCurveDisplay (Optimized Model):\")\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    RocCurveDisplay.from_estimator(optimized_model, X_test, y_test, ax=ax, name='Optimized Model')\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='navy', label='Chance', alpha=.8)\n",
    "    ax.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"\\nSkipping ROC Curve: ROC curve is typically for binary classification. For multi-class, consider one-vs-rest or micro/macro averaging AUC.\")\n",
    "\n",
    "\n",
    "# 5.3. Residual Plots (if it were a regression task)\n",
    "# This part is commented out as mental health analysis is often classification.\n",
    "# If your task is regression, uncomment and adapt.\n",
    "# if is_regression_task: # You'd define this flag earlier based on your target variable type\n",
    "#     print(\"\\nResidual Plots (Optimized Model - if regression):\")\n",
    "#     # Assuming 'y_pred_optimized' are continuous predictions and 'y_test' is continuous\n",
    "#     residuals = y_test - y_pred_optimized\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     sns.scatterplot(x=y_pred_optimized, y=residuals)\n",
    "#     plt.axhline(y=0, color='r', linestyle='--')\n",
    "#     plt.xlabel('Predicted Values')\n",
    "#     plt.ylabel('Residuals')\n",
    "#     plt.title('Residual Plot')\n",
    "#     plt.show()\n",
    "#\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     sns.histplot(residuals, kde=True)\n",
    "#     plt.xlabel('Residuals')\n",
    "#     plt.ylabel('Frequency')\n",
    "#     plt.title('Distribution of Residuals')\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# 5.4. Time-series prediction plots (if applicable)\n",
    "# This section assumes a time-series component, which is not directly evident from `mental_health_analysis.csv`.\n",
    "# If your data has a 'Date' or 'Time' column and you are performing time-series forecasting,\n",
    "# you would:\n",
    "# 1. Ensure your train/test split respects the time order.\n",
    "# 2. Plot actual vs. predicted values over time.\n",
    "# Example (conceptual):\n",
    "# if 'Date' in df.columns: # And you've done time-series splitting\n",
    "#     # Assuming 'X_test_original' stores the original index/time from X_test\n",
    "#     # and y_pred_optimized aligns with X_test_original's time order\n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     plt.plot(X_test_original['Date'], y_test, label='Actual')\n",
    "#     plt.plot(X_test_original['Date'], y_pred_optimized, label='Predicted')\n",
    "#     plt.xlabel('Date')\n",
    "#     plt.ylabel('Mental Health Condition')\n",
    "#     plt.title('Time Series Prediction (Optimized Model)')\n",
    "#     plt.legend()\n",
    "#     plt.xticks(rotation=45)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "print(\"\\nTime-series prediction plots are not generated as no time-series component was explicitly identified in the dataset for forecasting.\")\n",
    "\n",
    "\n",
    "# --- 6. Comparison of Baseline Model Performance with Optimized Model ---\n",
    "print(\"\\n--- Comparison of Baseline vs. Optimized Model Performance ---\")\n",
    "\n",
    "metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1-score'] # You can add more\n",
    "\n",
    "print(f\"{'Metric':<15} {'Baseline':<15} {'Optimized':<15}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "# For classification metrics (assuming 'weighted avg' for multi-class or overall accuracy)\n",
    "for metric in metrics_to_compare:\n",
    "    if metric == 'accuracy':\n",
    "        baseline_value = baseline_accuracy\n",
    "        optimized_value = optimized_accuracy\n",
    "    else:\n",
    "        # For precision, recall, f1-score, we often look at 'weighted avg' or 'macro avg' for multi-class\n",
    "        # For simplicity, taking 'weighted avg' if available, else 'accuracy'\n",
    "        baseline_value = baseline_report['weighted avg'][metric] if 'weighted avg' in baseline_report and metric in baseline_report['weighted avg'] else np.nan\n",
    "        optimized_value = optimized_report['weighted avg'][metric] if 'weighted avg' in optimized_report and metric in optimized_report['weighted avg'] else np.nan\n",
    "\n",
    "    print(f\"{metric:<15} {baseline_value:<15.4f} {optimized_value:<15.4f}\")\n",
    "\n",
    "# Visual comparison (Bar Chart)\n",
    "labels = ['Baseline', 'Optimized']\n",
    "accuracy_scores = [baseline_accuracy, optimized_accuracy]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "rects1 = ax.bar(x, accuracy_scores, width, label='Accuracy', color=['skyblue', 'lightcoral'])\n",
    "\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Accuracy Comparison: Baseline vs. Optimized')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.4f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n--- Task 4: Model Optimization & Advanced Evaluation Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2024.02-py310",
   "language": "python",
   "name": "conda-env-anaconda-2024.02-py310-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
